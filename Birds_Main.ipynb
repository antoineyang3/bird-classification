{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade Pillow\n",
    "!pip install tensorflow==1.11\n",
    "!pip install ipdb\n",
    "import ipdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim \n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import argparse\n",
    "import os\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Training settings\n",
    "class Args:\n",
    "  load_dir = 'drive/My Drive/Bird_Classification/features'\n",
    "  data = \"drive/My Drive/Bird_Classification/data\"\n",
    "  img_size=299\n",
    "  batch_size=32\n",
    "  epochs=50\n",
    "  lr=0.01\n",
    "  momentum=0.9\n",
    "  weight_decay=3e-4\n",
    "  grad_clip=5.\n",
    "  seed=1\n",
    "  log_interval=10\n",
    "  experiment='drive/My Drive/Bird_Classification/experiment'\n",
    "  outfile='drive/My Drive/experiment/kaggle.csv'\n",
    "  pth='drive/My Drive/Bird_Classification/experiment/InceptionV3_2_Layers/model_21_95.pth'\n",
    "\n",
    "args = Args()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Get files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Create experiment folder\n",
    "if not os.path.isdir(args.experiment):\n",
    "    os.makedirs(args.experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbl_rja_T88O"
   },
   "source": [
    "# Inception3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "MyrneWRkA4jt",
    "outputId": "88eb8360-16de-4ada-e866-4e41fa3f5feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Length of train: 1082\n",
      "Length of val: 103\n",
      "Length of val: 517\n",
      "2019-11-22 23:29:51.718028: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "Feature extraction on training set...\n",
      "0/1082 0.00s\n",
      "1000/1082 424.29s\n",
      "Feature extraction on validation set...\n",
      "0/103 459.33s\n",
      "Feature extraction on test set...\n",
      "0/517 503.54s\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction on original dataset with pre-trained on ImageNet and iNat2017 Inception3\n",
    "!python \"drive/My Drive/Bird_Classification/cvpr18-inaturalist-transfer-master/feature_extraction.py\" --dataset \"bird_dataset\" --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "5fkC2EHLFy_X",
    "outputId": "50fd3c35-d741-44f0-d1bb-7bd7bbe9d5d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Length of train: 1082\n",
      "Length of val: 103\n",
      "Length of val: 517\n",
      "2019-11-23 07:27:28.330006: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "Feature extraction on training set...\n",
      "0/1082 0.00s\n",
      "1000/1082 812.47s\n",
      "Feature extraction on validation set...\n",
      "0/103 877.04s\n",
      "Feature extraction on test set...\n",
      "0/517 959.87s\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction on cropped dataset with pre-trained on ImageNet and iNat2017 Inception3\n",
    "!python \"drive/My Drive/Bird_Classification/cvpr18-inaturalist-transfer-master/feature_extraction.py\" --dataset \"crop_dataset\" --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JtOwB8R80Dch"
   },
   "outputs": [],
   "source": [
    "# Additionnal test features extraction for test time augmentation\n",
    "additionnal_test_sets = ['flip_bird_dataset', 'flip_crop_bird_dataset']\n",
    "for test_set in additionnal_test_sets:\n",
    "  !python \"drive/My Drive/Bird_Classification/cvpr18-inaturalist-transfer-master/feature_extraction.py\" --dataset test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JzSHkXFougqd",
    "outputId": "81e6f791-dab3-48e7-d32b-3ba1e76a15e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "# 2-layers network of the features\n",
    "\n",
    "# Features and labels\n",
    "features_train = np.load(os.path.join(args.load_dir, 'bird_dataset_dense_feature_train.npy'))\n",
    "labels_train = np.load(os.path.join(args.load_dir, 'bird_dataset_dense_label_train.npy'))\n",
    "features_val = np.load(os.path.join(args.load_dir, 'bird_dataset_dense_feature_val.npy'))\n",
    "labels_val = np.load(os.path.join(args.load_dir, 'bird_dataset_dense_label_val.npy'))\n",
    "\n",
    "features_train_crop = np.load(os.path.join(args.load_dir, 'augmented_dataset_dense_feature_train.npy'))\n",
    "features_val_crop = np.load(os.path.join(args.load_dir, 'augmented_dataset_dense_feature_val.npy'))\n",
    "\n",
    "features_train = np.concatenate((features_train, features_train_crop), axis=1)\n",
    "features_val = np.concatenate((features_val, features_val_crop), axis=1)\n",
    "\n",
    "# Dataloaders\n",
    "features_tensor = torch.stack([torch.Tensor(i) for i in features_train]) \n",
    "labels_tensor = torch.stack([torch.Tensor([i]) for i in labels_train])\n",
    "train_data = torch.utils.data.TensorDataset(features_tensor, labels_tensor) \n",
    "features_tensor = torch.stack([torch.Tensor(i) for i in features_val])\n",
    "labels_tensor = torch.stack([torch.Tensor([i]) for i in labels_val])\n",
    "val_data = torch.utils.data.TensorDataset(features_tensor,labels_tensor) \n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self,embedding_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Classifier(features_train.shape[1])\n",
    "\n",
    "if use_cuda:\n",
    "    print('Using GPU')\n",
    "    model.cuda()\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "# Optimizer, LR, and criterion\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training functions\n",
    "def train_classifier(model, train_loader, optimizer, lr_scheduler, criterion, epoch):\n",
    "    lr_scheduler.step()\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "              data, target = Variable(data.cuda()), Variable(target.cuda().long())\n",
    "        else:\n",
    "              data, target = Variable(data), Variable(target.long())\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        target = target.squeeze(1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "\n",
    "def validation_classifier(model, criterion, val_loader):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            if use_cuda:\n",
    "              data, target = Variable(data.cuda()), Variable(target.cuda().long())\n",
    "            else:\n",
    "              data, target = Variable(data), Variable(target.long())\n",
    "            output = model(data)\n",
    "            target = target.squeeze(1)\n",
    "            # sum up batch loss\n",
    "            validation_loss += criterion(output, target).data.item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    validation_loss /= len(val_loader.dataset)\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        validation_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "    return(100. * correct / len(val_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zZD3aZjN0PxG",
    "outputId": "298704de-6e77-4057-e8de-a416289ceb12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1082 (0%)]\tLoss: 2.998858\n",
      "Train Epoch: 1 [320/1082 (29%)]\tLoss: 2.741377\n",
      "Train Epoch: 1 [640/1082 (59%)]\tLoss: 2.085440\n",
      "Train Epoch: 1 [960/1082 (88%)]\tLoss: 1.361625\n",
      "\n",
      "Validation set: Average loss: 0.0352, Accuracy: 92/103 (89%)\n",
      "Train Epoch: 2 [0/1082 (0%)]\tLoss: 0.676885\n",
      "Train Epoch: 2 [320/1082 (29%)]\tLoss: 0.323246\n",
      "Train Epoch: 2 [640/1082 (59%)]\tLoss: 0.266266\n",
      "Train Epoch: 2 [960/1082 (88%)]\tLoss: 0.237956\n",
      "\n",
      "Validation set: Average loss: 0.0085, Accuracy: 95/103 (92%)\n",
      "Train Epoch: 3 [0/1082 (0%)]\tLoss: 0.090857\n",
      "Train Epoch: 3 [320/1082 (29%)]\tLoss: 0.141260\n",
      "Train Epoch: 3 [640/1082 (59%)]\tLoss: 0.126691\n",
      "Train Epoch: 3 [960/1082 (88%)]\tLoss: 0.230064\n",
      "\n",
      "Validation set: Average loss: 0.0077, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 4 [0/1082 (0%)]\tLoss: 0.147218\n",
      "Train Epoch: 4 [320/1082 (29%)]\tLoss: 0.175938\n",
      "Train Epoch: 4 [640/1082 (59%)]\tLoss: 0.049514\n",
      "Train Epoch: 4 [960/1082 (88%)]\tLoss: 0.124837\n",
      "\n",
      "Validation set: Average loss: 0.0068, Accuracy: 95/103 (92%)\n",
      "Train Epoch: 5 [0/1082 (0%)]\tLoss: 0.242197\n",
      "Train Epoch: 5 [320/1082 (29%)]\tLoss: 0.139894\n",
      "Train Epoch: 5 [640/1082 (59%)]\tLoss: 0.175870\n",
      "Train Epoch: 5 [960/1082 (88%)]\tLoss: 0.090837\n",
      "\n",
      "Validation set: Average loss: 0.0079, Accuracy: 92/103 (89%)\n",
      "Train Epoch: 6 [0/1082 (0%)]\tLoss: 0.124634\n",
      "Train Epoch: 6 [320/1082 (29%)]\tLoss: 0.229739\n",
      "Train Epoch: 6 [640/1082 (59%)]\tLoss: 0.052910\n",
      "Train Epoch: 6 [960/1082 (88%)]\tLoss: 0.049096\n",
      "\n",
      "Validation set: Average loss: 0.0061, Accuracy: 97/103 (94%)\n",
      "Train Epoch: 7 [0/1082 (0%)]\tLoss: 0.047702\n",
      "Train Epoch: 7 [320/1082 (29%)]\tLoss: 0.106105\n",
      "Train Epoch: 7 [640/1082 (59%)]\tLoss: 0.115736\n",
      "Train Epoch: 7 [960/1082 (88%)]\tLoss: 0.050325\n",
      "\n",
      "Validation set: Average loss: 0.0063, Accuracy: 95/103 (92%)\n",
      "Train Epoch: 8 [0/1082 (0%)]\tLoss: 0.134744\n",
      "Train Epoch: 8 [320/1082 (29%)]\tLoss: 0.100336\n",
      "Train Epoch: 8 [640/1082 (59%)]\tLoss: 0.090545\n",
      "Train Epoch: 8 [960/1082 (88%)]\tLoss: 0.108361\n",
      "\n",
      "Validation set: Average loss: 0.0057, Accuracy: 97/103 (94%)\n",
      "Train Epoch: 9 [0/1082 (0%)]\tLoss: 0.031502\n",
      "Train Epoch: 9 [320/1082 (29%)]\tLoss: 0.070606\n",
      "Train Epoch: 9 [640/1082 (59%)]\tLoss: 0.095634\n",
      "Train Epoch: 9 [960/1082 (88%)]\tLoss: 0.033914\n",
      "\n",
      "Validation set: Average loss: 0.0061, Accuracy: 97/103 (94%)\n",
      "Train Epoch: 10 [0/1082 (0%)]\tLoss: 0.048224\n",
      "Train Epoch: 10 [320/1082 (29%)]\tLoss: 0.029972\n",
      "Train Epoch: 10 [640/1082 (59%)]\tLoss: 0.039159\n",
      "Train Epoch: 10 [960/1082 (88%)]\tLoss: 0.013457\n",
      "\n",
      "Validation set: Average loss: 0.0059, Accuracy: 97/103 (94%)\n",
      "Train Epoch: 11 [0/1082 (0%)]\tLoss: 0.071659\n",
      "Train Epoch: 11 [320/1082 (29%)]\tLoss: 0.029077\n",
      "Train Epoch: 11 [640/1082 (59%)]\tLoss: 0.063442\n",
      "Train Epoch: 11 [960/1082 (88%)]\tLoss: 0.092700\n",
      "\n",
      "Validation set: Average loss: 0.0063, Accuracy: 97/103 (94%)\n",
      "Train Epoch: 12 [0/1082 (0%)]\tLoss: 0.124557\n",
      "Train Epoch: 12 [320/1082 (29%)]\tLoss: 0.102088\n",
      "Train Epoch: 12 [640/1082 (59%)]\tLoss: 0.052907\n",
      "Train Epoch: 12 [960/1082 (88%)]\tLoss: 0.037195\n",
      "\n",
      "Validation set: Average loss: 0.0060, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 13 [0/1082 (0%)]\tLoss: 0.046842\n",
      "Train Epoch: 13 [320/1082 (29%)]\tLoss: 0.060102\n",
      "Train Epoch: 13 [640/1082 (59%)]\tLoss: 0.061735\n",
      "Train Epoch: 13 [960/1082 (88%)]\tLoss: 0.080615\n",
      "\n",
      "Validation set: Average loss: 0.0058, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 14 [0/1082 (0%)]\tLoss: 0.066283\n",
      "Train Epoch: 14 [320/1082 (29%)]\tLoss: 0.044292\n",
      "Train Epoch: 14 [640/1082 (59%)]\tLoss: 0.033736\n",
      "Train Epoch: 14 [960/1082 (88%)]\tLoss: 0.088948\n",
      "\n",
      "Validation set: Average loss: 0.0057, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 15 [0/1082 (0%)]\tLoss: 0.042020\n",
      "Train Epoch: 15 [320/1082 (29%)]\tLoss: 0.083736\n",
      "Train Epoch: 15 [640/1082 (59%)]\tLoss: 0.015636\n",
      "Train Epoch: 15 [960/1082 (88%)]\tLoss: 0.030514\n",
      "\n",
      "Validation set: Average loss: 0.0058, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 16 [0/1082 (0%)]\tLoss: 0.012198\n",
      "Train Epoch: 16 [320/1082 (29%)]\tLoss: 0.048171\n",
      "Train Epoch: 16 [640/1082 (59%)]\tLoss: 0.025848\n",
      "Train Epoch: 16 [960/1082 (88%)]\tLoss: 0.020604\n",
      "\n",
      "Validation set: Average loss: 0.0058, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 17 [0/1082 (0%)]\tLoss: 0.069582\n",
      "Train Epoch: 17 [320/1082 (29%)]\tLoss: 0.051723\n",
      "Train Epoch: 17 [640/1082 (59%)]\tLoss: 0.014813\n",
      "Train Epoch: 17 [960/1082 (88%)]\tLoss: 0.009804\n",
      "\n",
      "Validation set: Average loss: 0.0055, Accuracy: 97/103 (94%)\n",
      "Train Epoch: 18 [0/1082 (0%)]\tLoss: 0.043078\n",
      "Train Epoch: 18 [320/1082 (29%)]\tLoss: 0.039208\n",
      "Train Epoch: 18 [640/1082 (59%)]\tLoss: 0.034697\n",
      "Train Epoch: 18 [960/1082 (88%)]\tLoss: 0.018243\n",
      "\n",
      "Validation set: Average loss: 0.0056, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 19 [0/1082 (0%)]\tLoss: 0.017731\n",
      "Train Epoch: 19 [320/1082 (29%)]\tLoss: 0.060467\n",
      "Train Epoch: 19 [640/1082 (59%)]\tLoss: 0.057034\n",
      "Train Epoch: 19 [960/1082 (88%)]\tLoss: 0.026134\n",
      "\n",
      "Validation set: Average loss: 0.0066, Accuracy: 95/103 (92%)\n",
      "Train Epoch: 20 [0/1082 (0%)]\tLoss: 0.051480\n",
      "Train Epoch: 20 [320/1082 (29%)]\tLoss: 0.067784\n",
      "Train Epoch: 20 [640/1082 (59%)]\tLoss: 0.010730\n",
      "Train Epoch: 20 [960/1082 (88%)]\tLoss: 0.026584\n",
      "\n",
      "Validation set: Average loss: 0.0060, Accuracy: 95/103 (92%)\n",
      "Train Epoch: 21 [0/1082 (0%)]\tLoss: 0.019161\n",
      "Train Epoch: 21 [320/1082 (29%)]\tLoss: 0.074583\n",
      "Train Epoch: 21 [640/1082 (59%)]\tLoss: 0.035845\n",
      "Train Epoch: 21 [960/1082 (88%)]\tLoss: 0.027042\n",
      "\n",
      "Validation set: Average loss: 0.0057, Accuracy: 98/103 (95%)\n",
      "Saved model to drive/My Drive/Bird_Classification/experiment/model_21.pth. You can run `python evaluate.py --model drive/My Drive/Bird_Classification/experiment/model_21.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 22 [0/1082 (0%)]\tLoss: 0.021118\n",
      "Train Epoch: 22 [320/1082 (29%)]\tLoss: 0.048435\n",
      "Train Epoch: 22 [640/1082 (59%)]\tLoss: 0.040113\n",
      "Train Epoch: 22 [960/1082 (88%)]\tLoss: 0.021953\n",
      "\n",
      "Validation set: Average loss: 0.0056, Accuracy: 95/103 (92%)\n",
      "Train Epoch: 23 [0/1082 (0%)]\tLoss: 0.011003\n",
      "Train Epoch: 23 [320/1082 (29%)]\tLoss: 0.014244\n",
      "Train Epoch: 23 [640/1082 (59%)]\tLoss: 0.033518\n",
      "Train Epoch: 23 [960/1082 (88%)]\tLoss: 0.012262\n",
      "\n",
      "Validation set: Average loss: 0.0059, Accuracy: 94/103 (91%)\n",
      "Train Epoch: 24 [0/1082 (0%)]\tLoss: 0.035498\n",
      "Train Epoch: 24 [320/1082 (29%)]\tLoss: 0.027380\n",
      "Train Epoch: 24 [640/1082 (59%)]\tLoss: 0.017783\n",
      "Train Epoch: 24 [960/1082 (88%)]\tLoss: 0.020677\n",
      "\n",
      "Validation set: Average loss: 0.0057, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 25 [0/1082 (0%)]\tLoss: 0.034423\n",
      "Train Epoch: 25 [320/1082 (29%)]\tLoss: 0.029582\n",
      "Train Epoch: 25 [640/1082 (59%)]\tLoss: 0.023213\n",
      "Train Epoch: 25 [960/1082 (88%)]\tLoss: 0.006199\n",
      "\n",
      "Validation set: Average loss: 0.0056, Accuracy: 97/103 (94%)\n",
      "Train Epoch: 26 [0/1082 (0%)]\tLoss: 0.006471\n",
      "Train Epoch: 26 [320/1082 (29%)]\tLoss: 0.014009\n",
      "Train Epoch: 26 [640/1082 (59%)]\tLoss: 0.019081\n",
      "Train Epoch: 26 [960/1082 (88%)]\tLoss: 0.011753\n",
      "\n",
      "Validation set: Average loss: 0.0061, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 27 [0/1082 (0%)]\tLoss: 0.005628\n",
      "Train Epoch: 27 [320/1082 (29%)]\tLoss: 0.013893\n",
      "Train Epoch: 27 [640/1082 (59%)]\tLoss: 0.032604\n",
      "Train Epoch: 27 [960/1082 (88%)]\tLoss: 0.058009\n",
      "\n",
      "Validation set: Average loss: 0.0059, Accuracy: 95/103 (92%)\n",
      "Train Epoch: 28 [0/1082 (0%)]\tLoss: 0.015424\n",
      "Train Epoch: 28 [320/1082 (29%)]\tLoss: 0.038393\n",
      "Train Epoch: 28 [640/1082 (59%)]\tLoss: 0.006066\n",
      "Train Epoch: 28 [960/1082 (88%)]\tLoss: 0.015057\n",
      "\n",
      "Validation set: Average loss: 0.0060, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 29 [0/1082 (0%)]\tLoss: 0.006954\n",
      "Train Epoch: 29 [320/1082 (29%)]\tLoss: 0.008195\n",
      "Train Epoch: 29 [640/1082 (59%)]\tLoss: 0.032529\n",
      "Train Epoch: 29 [960/1082 (88%)]\tLoss: 0.014340\n",
      "\n",
      "Validation set: Average loss: 0.0060, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 30 [0/1082 (0%)]\tLoss: 0.012146\n",
      "Train Epoch: 30 [320/1082 (29%)]\tLoss: 0.020995\n",
      "Train Epoch: 30 [640/1082 (59%)]\tLoss: 0.004124\n",
      "Train Epoch: 30 [960/1082 (88%)]\tLoss: 0.029356\n",
      "\n",
      "Validation set: Average loss: 0.0065, Accuracy: 95/103 (92%)\n",
      "Train Epoch: 31 [0/1082 (0%)]\tLoss: 0.010089\n",
      "Train Epoch: 31 [320/1082 (29%)]\tLoss: 0.011790\n",
      "Train Epoch: 31 [640/1082 (59%)]\tLoss: 0.038505\n",
      "Train Epoch: 31 [960/1082 (88%)]\tLoss: 0.031465\n",
      "\n",
      "Validation set: Average loss: 0.0063, Accuracy: 95/103 (92%)\n",
      "Train Epoch: 32 [0/1082 (0%)]\tLoss: 0.021873\n",
      "Train Epoch: 32 [320/1082 (29%)]\tLoss: 0.026229\n",
      "Train Epoch: 32 [640/1082 (59%)]\tLoss: 0.021827\n",
      "Train Epoch: 32 [960/1082 (88%)]\tLoss: 0.034346\n",
      "\n",
      "Validation set: Average loss: 0.0060, Accuracy: 95/103 (92%)\n",
      "Train Epoch: 33 [0/1082 (0%)]\tLoss: 0.017945\n",
      "Train Epoch: 33 [320/1082 (29%)]\tLoss: 0.010187\n",
      "Train Epoch: 33 [640/1082 (59%)]\tLoss: 0.018311\n",
      "Train Epoch: 33 [960/1082 (88%)]\tLoss: 0.022058\n",
      "\n",
      "Validation set: Average loss: 0.0061, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 34 [0/1082 (0%)]\tLoss: 0.016751\n",
      "Train Epoch: 34 [320/1082 (29%)]\tLoss: 0.031112\n",
      "Train Epoch: 34 [640/1082 (59%)]\tLoss: 0.010287\n",
      "Train Epoch: 34 [960/1082 (88%)]\tLoss: 0.006812\n",
      "\n",
      "Validation set: Average loss: 0.0060, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 35 [0/1082 (0%)]\tLoss: 0.015874\n",
      "Train Epoch: 35 [320/1082 (29%)]\tLoss: 0.010634\n",
      "Train Epoch: 35 [640/1082 (59%)]\tLoss: 0.020851\n",
      "Train Epoch: 35 [960/1082 (88%)]\tLoss: 0.019935\n",
      "\n",
      "Validation set: Average loss: 0.0061, Accuracy: 95/103 (92%)\n",
      "Train Epoch: 36 [0/1082 (0%)]\tLoss: 0.010469\n",
      "Train Epoch: 36 [320/1082 (29%)]\tLoss: 0.024226\n",
      "Train Epoch: 36 [640/1082 (59%)]\tLoss: 0.031681\n",
      "Train Epoch: 36 [960/1082 (88%)]\tLoss: 0.023545\n",
      "\n",
      "Validation set: Average loss: 0.0062, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 37 [0/1082 (0%)]\tLoss: 0.041445\n",
      "Train Epoch: 37 [320/1082 (29%)]\tLoss: 0.012685\n",
      "Train Epoch: 37 [640/1082 (59%)]\tLoss: 0.017050\n",
      "Train Epoch: 37 [960/1082 (88%)]\tLoss: 0.016393\n",
      "\n",
      "Validation set: Average loss: 0.0059, Accuracy: 95/103 (92%)\n",
      "Train Epoch: 38 [0/1082 (0%)]\tLoss: 0.026480\n",
      "Train Epoch: 38 [320/1082 (29%)]\tLoss: 0.021162\n",
      "Train Epoch: 38 [640/1082 (59%)]\tLoss: 0.015514\n",
      "Train Epoch: 38 [960/1082 (88%)]\tLoss: 0.016945\n",
      "\n",
      "Validation set: Average loss: 0.0061, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 39 [0/1082 (0%)]\tLoss: 0.008677\n",
      "Train Epoch: 39 [320/1082 (29%)]\tLoss: 0.012119\n",
      "Train Epoch: 39 [640/1082 (59%)]\tLoss: 0.015868\n",
      "Train Epoch: 39 [960/1082 (88%)]\tLoss: 0.019349\n",
      "\n",
      "Validation set: Average loss: 0.0061, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 40 [0/1082 (0%)]\tLoss: 0.013807\n",
      "Train Epoch: 40 [320/1082 (29%)]\tLoss: 0.013422\n",
      "Train Epoch: 40 [640/1082 (59%)]\tLoss: 0.011630\n",
      "Train Epoch: 40 [960/1082 (88%)]\tLoss: 0.010767\n",
      "\n",
      "Validation set: Average loss: 0.0060, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 41 [0/1082 (0%)]\tLoss: 0.010160\n",
      "Train Epoch: 41 [320/1082 (29%)]\tLoss: 0.013435\n",
      "Train Epoch: 41 [640/1082 (59%)]\tLoss: 0.007868\n",
      "Train Epoch: 41 [960/1082 (88%)]\tLoss: 0.013974\n",
      "\n",
      "Validation set: Average loss: 0.0060, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 42 [0/1082 (0%)]\tLoss: 0.012177\n",
      "Train Epoch: 42 [320/1082 (29%)]\tLoss: 0.007654\n",
      "Train Epoch: 42 [640/1082 (59%)]\tLoss: 0.013453\n",
      "Train Epoch: 42 [960/1082 (88%)]\tLoss: 0.004073\n",
      "\n",
      "Validation set: Average loss: 0.0060, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 43 [0/1082 (0%)]\tLoss: 0.014765\n",
      "Train Epoch: 43 [320/1082 (29%)]\tLoss: 0.028998\n",
      "Train Epoch: 43 [640/1082 (59%)]\tLoss: 0.016890\n",
      "Train Epoch: 43 [960/1082 (88%)]\tLoss: 0.015090\n",
      "\n",
      "Validation set: Average loss: 0.0061, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 44 [0/1082 (0%)]\tLoss: 0.020018\n",
      "Train Epoch: 44 [320/1082 (29%)]\tLoss: 0.012955\n",
      "Train Epoch: 44 [640/1082 (59%)]\tLoss: 0.011884\n",
      "Train Epoch: 44 [960/1082 (88%)]\tLoss: 0.008675\n",
      "\n",
      "Validation set: Average loss: 0.0061, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 45 [0/1082 (0%)]\tLoss: 0.008324\n",
      "Train Epoch: 45 [320/1082 (29%)]\tLoss: 0.018416\n",
      "Train Epoch: 45 [640/1082 (59%)]\tLoss: 0.022747\n",
      "Train Epoch: 45 [960/1082 (88%)]\tLoss: 0.013734\n",
      "\n",
      "Validation set: Average loss: 0.0061, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 46 [0/1082 (0%)]\tLoss: 0.033847\n",
      "Train Epoch: 46 [320/1082 (29%)]\tLoss: 0.016443\n",
      "Train Epoch: 46 [640/1082 (59%)]\tLoss: 0.014445\n",
      "Train Epoch: 46 [960/1082 (88%)]\tLoss: 0.016319\n",
      "\n",
      "Validation set: Average loss: 0.0060, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 47 [0/1082 (0%)]\tLoss: 0.021663\n",
      "Train Epoch: 47 [320/1082 (29%)]\tLoss: 0.015198\n",
      "Train Epoch: 47 [640/1082 (59%)]\tLoss: 0.016723\n",
      "Train Epoch: 47 [960/1082 (88%)]\tLoss: 0.004929\n",
      "\n",
      "Validation set: Average loss: 0.0060, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 48 [0/1082 (0%)]\tLoss: 0.005836\n",
      "Train Epoch: 48 [320/1082 (29%)]\tLoss: 0.011291\n",
      "Train Epoch: 48 [640/1082 (59%)]\tLoss: 0.018376\n",
      "Train Epoch: 48 [960/1082 (88%)]\tLoss: 0.003523\n",
      "\n",
      "Validation set: Average loss: 0.0060, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 49 [0/1082 (0%)]\tLoss: 0.006359\n",
      "Train Epoch: 49 [320/1082 (29%)]\tLoss: 0.023713\n",
      "Train Epoch: 49 [640/1082 (59%)]\tLoss: 0.019759\n",
      "Train Epoch: 49 [960/1082 (88%)]\tLoss: 0.015209\n",
      "\n",
      "Validation set: Average loss: 0.0060, Accuracy: 96/103 (93%)\n",
      "Train Epoch: 50 [0/1082 (0%)]\tLoss: 0.010145\n",
      "Train Epoch: 50 [320/1082 (29%)]\tLoss: 0.009281\n",
      "Train Epoch: 50 [640/1082 (59%)]\tLoss: 0.025746\n",
      "Train Epoch: 50 [960/1082 (88%)]\tLoss: 0.031053\n",
      "\n",
      "Validation set: Average loss: 0.0060, Accuracy: 96/103 (93%)\n"
     ]
    }
   ],
   "source": [
    "# Training the classifier \n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train_classifier(epoch)\n",
    "    val_acc=validation_classifier()\n",
    "    if val_acc>=95:\n",
    "      # Save only when it is good enough\n",
    "      model_file = args.experiment + '/model_' + str(epoch) + '.pth'\n",
    "      torch.save(model.state_dict(), model_file)\n",
    "      print('Saved model to ' + model_file + '. You can run `python evaluate.py --model ' + model_file + '` to generate the Kaggle formatted csv file\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "BX1Z4c0RQZwI",
    "outputId": "435c3b87-7589-4865-93b1-236d88e10c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully wrote kaggle.csv, you can upload this file to the kaggle competition website\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "# Test features\n",
    "features_test = np.load(os.path.join(args.load_dir, 'bird_dataset_dense_feature_test.npy'))\n",
    "features_test_crop = np.load(os.path.join(args.load_dir, 'augmented_dataset_dense_feature_test.npy'))\n",
    "features_test = np.concatenate((features_test, features_test_crop), axis=1)\n",
    "\n",
    "# Loading trained model\n",
    "state_dict = torch.load(args.pth)\n",
    "model = Classifier(features_test.shape[1])\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "if use_cuda:\n",
    "    print('Using GPU')\n",
    "    model.cuda()\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "# Test \n",
    "df = pd.read_csv('drive/My Drive/Bird_Classification/template.csv')\n",
    "test_list=[]\n",
    "for line in open(os.path.join(args.data+ '/flip_bird_dataset/test.txt'), 'r'):\n",
    "    test_list.append(line[:-1])\n",
    "    \n",
    "for i in range(len(test_list)):\n",
    "    img_idx = df[df['Id'] == test_list[i][:-4]].index[0]\n",
    "    data = torch.tensor(features_test[i])\n",
    "    if use_cuda:\n",
    "        data = data.cuda()\n",
    "    output = model(data)\n",
    "    pred = torch.max(output.data, 0)[1]\n",
    "    df.Category[img_idx] = pred.cpu().data.numpy()\n",
    "\n",
    "df.to_csv('drive/My Drive/Bird_Classification/kaggle.csv',index=False)\n",
    "print(\"Succesfully wrote kaggle.csv, you can upload this file to the kaggle competition website\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "JEmpYnUw1MGU",
    "outputId": "628b1d85-1cbf-49f8-8eb6-4ff87f85d2d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully wrote kaggle.csv, you can upload this file to the kaggle competition website\n"
     ]
    }
   ],
   "source": [
    "# Test with Test Time Augmentation\n",
    "\n",
    "# Test features\n",
    "features_test = np.load(os.path.join(args.load_dir, 'bird_dataset_dense_feature_test.npy'))\n",
    "features_test_crop = np.load(os.path.join(args.load_dir, 'augmented_dataset_dense_feature_test.npy'))\n",
    "features_test = np.concatenate((features_test, features_test_crop), axis=1)\n",
    "\n",
    "# Additional test features\n",
    "features_test_flip = np.load(os.path.join(args.load_dir, 'flip_bird_dataset_feature_test.npy'))\n",
    "features_test_crop_flip = np.load(os.path.join(args.load_dir, 'flip_crop_bird_dataset_feature_test.npy'))\n",
    "features_test_flip = np.concatenate((features_test_flip, features_test_crop_flip), axis=1)\n",
    "\n",
    "# Loading trained model\n",
    "state_dict = torch.load(args.pth)\n",
    "model = Classifier(features_test.shape[1])\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "if use_cuda:\n",
    "    print('Using GPU')\n",
    "    model.cuda()\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "# Test \n",
    "df = pd.read_csv('drive/My Drive/Bird_Classification/template.csv')\n",
    "test_list=[]\n",
    "for line in open(os.path.join(args.data+ '/flip_bird_dataset/test.txt'), 'r'):\n",
    "    test_list.append(line[:-1])\n",
    "  \n",
    "norm = torch.nn.Softmax(0)\n",
    "for i in range(len(test_list)):\n",
    "    img_idx = df[df['Id'] == test_list[i][:-4]].index[0]\n",
    "    data = torch.tensor(features_test[i])\n",
    "    data_flip = torch.tensor(features_test_flip[i])\n",
    "    if use_cuda:\n",
    "        data = data.cuda()\n",
    "        data_flip = data_flip.cuda()\n",
    "    output = norm(model(data))\n",
    "    output_flip = norm(model(data_flip))\n",
    "    # max TTA\n",
    "    confidence1, pred1 = torch.max(output.data, 0)\n",
    "    confidence2, pred2 = torch.max(output_flip.data, 0)\n",
    "    index = torch.max(torch.stack((confidence1,confidence2)),0)[1].cpu().data.numpy().item(0)\n",
    "    pred = torch.stack((pred1,pred2))[index]\n",
    "    # average TTA\n",
    "    # pred = torch.max(output.data+output_flip.data, 0)[1]\n",
    "    df.Category[img_idx] = pred.cpu().data.numpy()\n",
    "\n",
    "df.to_csv('drive/My Drive/Bird_Classification/kaggle.csv',index=False)\n",
    "print(\"Succesfully wrote kaggle.csv, you can upload this file to the kaggle competition website\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Birds_Main.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
